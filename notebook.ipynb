{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "79a67c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langchain langchain-community pillow pymupdf python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c2483536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "from langchain_huggingface.llms.huggingface_endpoint import HuggingFaceEndpoint\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "537310a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = []\n",
    "img_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e73e6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with fitz.open('training_documents/Survey on Tabular Data.pdf') as pdf_file:\n",
    "    # Create a directory to store the images\n",
    "    if not os.path.exists(\"extracted_images\"):\n",
    "        os.makedirs(\"extracted_images\")\n",
    "    \n",
    "     # Loop through every page in the PDF\n",
    "    for page_number in range(len(pdf_file)):\n",
    "        page = pdf_file[page_number]\n",
    "        \n",
    "        # Get the text on page\n",
    "        text = page.get_text().strip()\n",
    "        text_data.append({\"response\": text, \"name\": page_number+1})\n",
    "        # Get the list of images on the page\n",
    "        images = page.get_images(full=True)\n",
    "\n",
    "        # Loop through all images found on the page\n",
    "        for image_index, img in enumerate(images, start=0):\n",
    "            xref = img[0]  # Get the XREF of the image\n",
    "            base_image = pdf_file.extract_image(xref)  # Extract the image\n",
    "            image_bytes = base_image[\"image\"]  # Get the image bytes\n",
    "            image_ext = base_image[\"ext\"]  # Get the image extension\n",
    "            \n",
    "            # Load the image using PIL and save it\n",
    "            image = Image.open(io.BytesIO(image_bytes))\n",
    "            image.save(f\"extracted_images/image_{page_number+1}_{image_index+1}.{image_ext}\")    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e1533a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "genai.configure(api_key=api_key)\n",
    "model = genai.GenerativeModel(model_name=\"gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fff115f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in os.listdir(\"extracted_images\"):\n",
    "    image = Image.open(f\"extracted_images/{img}\")\n",
    "    response = model.generate_content([image, \"You are an AI assistant helping build a retrieval system from academic papers. The input is a table or figure image extracted from a paper. \\\n",
    "                                            Summarize the image with reference to the core topic or claim being visualized. Include comparisons, axes, legends, and what this visual proves or supports in context of the paper. \\\n",
    "                                            Your summary will be embedded and must serve as a high-quality retrieval chunk. Be specific, concise, and factually grounded.\"\n",
    "])\n",
    "    img_data.append({\"response\": response.text, \"name\": img})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9b7d88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# Load the document\n",
    "docs_list = [Document(page_content=text['response'], metadata={\"name\": text['name']}) for text in text_data]\n",
    "img_list = [Document(page_content=img['response'], metadata={\"name\": img['name']}) for img in img_data]\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=400, chunk_overlap=50\n",
    ")\n",
    "\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "img_splits = text_splitter.split_documents(img_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb9b7039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to vectorstore\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits + img_splits, # adding the both text and image splits\n",
    "    collection_name=\"multi_model_rag\",\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "                search_type=\"similarity\",\n",
    "                search_kwargs={'k': 1}, # number of documents to retrieve\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbbebe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    \"Interpret Figure 5: The data generation process for causual LMs from Survey on Tabular Data \"\n",
    "    \"what do the boxes and arrows represent, and how do they illustrate knowledge transfer \"\n",
    "    \"from the source to the target domain?\"\n",
    ")  \n",
    "docs = retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c30cc3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a summary of the image:\n",
      "\n",
      "This image illustrates a process of generating textual descriptions from structured table data using preconditioning prompts. The top table shows the original data with the columns: Age, Education, Job, and Marital. The \"Preconditioning prompts\" section indicates three example inputs which affect the generated textual description. The bottom table shows new data that is used to generate the output in the \"Generation\" section. The blue arrows indicate the flow of the process from Cell value extraction to Generation. The generated text from the bottom table follows the examples in the \"Preconditioning prompts\" section.\n",
      "{'name': 'image_20_1.png'}\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ae3346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Assistant: Figure 5 illustrates a process for generating textual descriptions from structured table data using preconditioning prompts. The boxes represent different stages in the process, while the arrows indicate the flow of data between these stages.\n",
      "\n",
      "1. **Cell value extraction**: This stage involves extracting specific values from the table data, such as Age, Education, Job, and Marital status. These values are represented by the boxes labeled with these categories.\n",
      "\n",
      "2. **Preconditioning prompts**: This stage involves providing specific inputs or prompts that influence the generated textual description. The example inputs provided in the image are \"A young person\", \"A highly educated individual\", and \"A married person\". These prompts are represented by the boxes labeled with these descriptions.\n",
      "\n",
      "3. **Generation**: This stage involves using the extracted cell values and the preconditioning prompts to generate a textual description. The generated text is represented by the boxes labeled with the generated descriptions.\n",
      "\n",
      "The knowledge transfer from the source to the target domain occurs through the use of preconditioning prompts. By providing specific inputs that are relevant to the target domain (e.g., \"A young person\" for a dataset about students),\n"
     ]
    }
   ],
   "source": [
    "# Define system prompt\n",
    "system = \"\"\"You are an assistant for QA based on academic papers.\n",
    "Always ground your answer strictly in the retrieved evidence belowâ€”no outside knowledge.\n",
    "If evidence is from an image or table summary, reference the figure/table number if present.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system),\n",
    "    (\"human\", \"Retrieved documents:\\n\\n<docs>{documents}</docs>\\n\\nUser question:\\n{question}\")\n",
    "])\n",
    "\n",
    "# Initialize LLM with conversational task\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\",  \n",
    "    huggingfacehub_api_token=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"),\n",
    "    task=\"conversational\",\n",
    "    temperature=0.05,\n",
    "    max_new_tokens=256,\n",
    "    timeout=300 \n",
    ")\n",
    "\n",
    "# Build the chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Retrieve documents\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "docs_joined = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Run the chain\n",
    "generation = rag_chain.invoke({\n",
    "    \"documents\": docs_joined,\n",
    "    \"question\": query\n",
    "})\n",
    "\n",
    "print(generation)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
