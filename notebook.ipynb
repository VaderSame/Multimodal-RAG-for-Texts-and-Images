{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03d8e256",
   "metadata": {},
   "source": [
    "\n",
    "    PyMuPDF: For extracting text and images from PDFs.\n",
    "    Gemini 1.5-flash model: To summarize images and tables.\n",
    "    Cohere Embeddings: For embedding document splits.\n",
    "    Chroma Vectorstore: To store and retrieve document embeddings.\n",
    "    LangChain: To orchestrate the retrieval and generation pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "79a67c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langchain langchain-community pillow pymupdf python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c2483536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "# from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "537310a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = []\n",
    "img_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6e73e6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with fitz.open('training_documents/Transfer Learning.pdf') as pdf_file:\n",
    "    # Create a directory to store the images\n",
    "    if not os.path.exists(\"extracted_images\"):\n",
    "        os.makedirs(\"extracted_images\")\n",
    "    \n",
    "     # Loop through every page in the PDF\n",
    "    for page_number in range(len(pdf_file)):\n",
    "        page = pdf_file[page_number]\n",
    "        \n",
    "        # Get the text on page\n",
    "        text = page.get_text().strip()\n",
    "        text_data.append({\"response\": text, \"name\": page_number+1})\n",
    "        # Get the list of images on the page\n",
    "        images = page.get_images(full=True)\n",
    "\n",
    "        # Loop through all images found on the page\n",
    "        for image_index, img in enumerate(images, start=0):\n",
    "            xref = img[0]  # Get the XREF of the image\n",
    "            base_image = pdf_file.extract_image(xref)  # Extract the image\n",
    "            image_bytes = base_image[\"image\"]  # Get the image bytes\n",
    "            image_ext = base_image[\"ext\"]  # Get the image extension\n",
    "            \n",
    "            # Load the image using PIL and save it\n",
    "            image = Image.open(io.BytesIO(image_bytes))\n",
    "            image.save(f\"extracted_images/image_{page_number+1}_{image_index+1}.{image_ext}\")    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1e1533a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "genai.configure(api_key=api_key)\n",
    "model = genai.GenerativeModel(model_name=\"gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fff115f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in os.listdir(\"extracted_images\"):\n",
    "    image = Image.open(f\"extracted_images/{img}\")\n",
    "    response = model.generate_content([image, \"You are an AI assistant helping build a retrieval system from academic papers. The input is a table or figure image extracted from a paper. \\\n",
    "                                            Summarize the image with reference to the core topic or claim being visualized. Include comparisons, axes, legends, and what this visual proves or supports in context of the paper. \\\n",
    "                                            Your summary will be embedded and must serve as a high-quality retrieval chunk. Be specific, concise, and factually grounded.\"\n",
    "])\n",
    "    img_data.append({\"response\": response.text, \"name\": img})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e9b7d88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# Load the document\n",
    "docs_list = [Document(page_content=text['response'], metadata={\"name\": text['name']}) for text in text_data]\n",
    "img_list = [Document(page_content=img['response'], metadata={\"name\": img['name']}) for img in img_data]\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=400, chunk_overlap=50\n",
    ")\n",
    "\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "img_splits = text_splitter.split_documents(img_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cb9b7039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to vectorstore\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits + img_splits, # adding the both text and image splits\n",
    "    collection_name=\"multi_model_rag\",\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "                search_type=\"similarity\",\n",
    "                search_kwargs={'k': 1}, # number of documents to retrieve\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fbbebe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    \"Interpret Figure 3 from the Transfer Learning PDF: \"\n",
    "    \"what do the boxes and arrows represent, and how do they illustrate knowledge transfer \"\n",
    "    \"from the source to the target domain?\"\n",
    ")  \n",
    "docs = retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c30cc3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A PREPRINT - JANUARY 14, 2025\n",
      "Figure 3: Text prompt template for transfer learning using DistilGPT2 [22].\n",
      "(a) All LLM weights are trainable\n",
      "(b) All LLM weights are frozen\n",
      "Figure 4: Convergence plots of proposed transfer learning of tabular data using a large language model (LLM)\n",
      "Although in-context learning via FeatLLM uses one of the most up-to-date LLMs, its performance may be significantly\n",
      "limited by the maximum token size. In-context learning via LLM API does not involve any finetuning or transfer\n",
      "learning. Without transfer learning, text input prompts (0.733 (0.021)) are able to achieve a better performance than the\n",
      "best overall GBT model (0.711(0.039)) on the blood transfusion data set. The shot and sample ratio (ssr) is 60% for this\n",
      "data set. The performance of in-context learning is on par with GBT on the dermatology (ssr: 11.5%), breast cancer\n",
      "(ssr: 8.4%), and diabetes (31%) data sets. The LLM may have some prior knowledge about these medical domains or\n",
      "data sets, which may have contributed to promising results despite few-shot learning.\n",
      "Transfer learning using an LLM with all frozen weights (Frozen) appears as the worst classification solution for tabular\n",
      "data. The performance drop may be attributed to the distilled LLM compared to the one used for in-context learning\n",
      "with a detailed input prompt. The rigidness of frozen weight is then fully relaxed to perform end-to-end finetuning\n",
      "of the LLM (End-to-end), which has substantially improved the average rank ordering from 6.0 (0.0) to 3.5 (1.3) on\n",
      "{'name': 6}\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b7bfcea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 3 illustrates the process of transfer learning using a Large Language Model (LLM). The boxes represent different components or steps in the process, and the arrows indicate the direction of knowledge transfer and the flow of information.\n",
      "\n",
      "Here's a breakdown of the elements in the figure:\n",
      "\n",
      "- The leftmost box represents the \"Source Domain,\" which contains the knowledge and information from the pre-trained LLM. This box has an arrow pointing to the right, indicating the transfer of knowledge to the target domain.\n",
      "- The rightmost box is the \"Target Domain,\" which is the specific task or domain to which we want to apply the knowledge from the source domain.\n",
      "- The arrows connecting the two domains represent the knowledge transfer process. The arrow from the source to the target indicates that the knowledge learned from the source domain is being applied to the target domain.\n",
      "- The arrow from the target domain back to the source domain indicates that the target task also influences the source domain's knowledge representation. This feedback loop allows for fine-tuning and adjustment of the pre-trained LLM weights based on the target task.\n",
      "- The boxes with dashed lines represent optional components or variations in the transfer learning process. In (a), all LLM weights are trainable, allowing for end-to-end fine-tuning of the model. In (b), all LLM weights are frozen, meaning that only the additional layers added on top of the LLM are trained, while the original LLM weights remain fixed.\n",
      "\n",
      "In summary, Figure 3 illustrates the process of knowledge transfer in transfer learning. The source domain's knowledge is applied to the target domain, and the arrows indicate the direction of information flow and the adjustment of the LLM weights based on the target task. The optional variations in (a) and (b) represent different approaches to fine-tuning the model during the knowledge transfer process.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_cohere import ChatCohere\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are an assistant for question-answering tasks based on academic papers. \n",
    "Always ground your answer strictly in the retrieved evidence below—do not use outside knowledge.\n",
    "If evidence is from an image or table summary, reference the figure or table number if present.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved documents: \\n\\n <docs>{documents}</docs> \\n\\n User question: <question>{question}</question>\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# LLM\n",
    "llm = ChatCohere(\n",
    "    model=\"command-r-plus\",\n",
    "    temperature=0,\n",
    "    \n",
    ")\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "generation = rag_chain.invoke({\"documents\":docs[0].page_content, \"question\": query})\n",
    "print(generation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
