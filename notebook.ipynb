{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79a67c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langchain langchain-community pillow pymupdf python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2483536",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "from langchain_huggingface.llms.huggingface_endpoint import HuggingFaceEndpoint\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "537310a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = []\n",
    "img_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6e73e6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with fitz.open('training_documents/Survey on Tabular Data.pdf') as pdf_file:\n",
    "    # Create a directory to store the images\n",
    "    if not os.path.exists(\"extracted_images\"):\n",
    "        os.makedirs(\"extracted_images\")\n",
    "    \n",
    "     # Loop through every page in the PDF\n",
    "    for page_number in range(len(pdf_file)):\n",
    "        page = pdf_file[page_number]\n",
    "        \n",
    "        # Get the text on page\n",
    "        text = page.get_text().strip()\n",
    "        text_data.append({\"response\": text, \"name\": page_number+1})\n",
    "        # Get the list of images on the page\n",
    "        images = page.get_images(full=True)\n",
    "\n",
    "        # Loop through all images found on the page\n",
    "        for image_index, img in enumerate(images, start=0):\n",
    "            xref = img[0]  # Get the XREF of the image\n",
    "            base_image = pdf_file.extract_image(xref)  # Extract the image\n",
    "            image_bytes = base_image[\"image\"]  # Get the image bytes\n",
    "            image_ext = base_image[\"ext\"]  # Get the image extension\n",
    "            \n",
    "            # Load the image using PIL and save it\n",
    "            image = Image.open(io.BytesIO(image_bytes))\n",
    "            image.save(f\"extracted_images/image_{page_number+1}_{image_index+1}.{image_ext}\")    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e1533a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "genai.configure(api_key=api_key)\n",
    "model = genai.GenerativeModel(model_name=\"gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fff115f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in os.listdir(\"extracted_images\"):\n",
    "    image = Image.open(f\"extracted_images/{img}\")\n",
    "    response = model.generate_content([image, \"You are an AI assistant helping build a retrieval system from academic papers. The input is a table or figure image extracted from a paper. \\\n",
    "                                            Summarize the image with reference to the core topic or claim being visualized. Include comparisons, axes, legends, and what this visual proves or supports in context of the paper. \\\n",
    "                                            Your summary will be embedded and must serve as a high-quality retrieval chunk. Be specific, concise, and factually grounded.\"\n",
    "])\n",
    "    img_data.append({\"response\": response.text, \"name\": img})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9b7d88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# Load the document\n",
    "docs_list = [Document(page_content=text['response'], metadata={\"name\": text['name']}) for text in text_data]\n",
    "img_list = [Document(page_content=img['response'], metadata={\"name\": img['name']}) for img in img_data]\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=400, chunk_overlap=50\n",
    ")\n",
    "\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "img_splits = text_splitter.split_documents(img_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb9b7039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to vectorstore\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits + img_splits, # adding the both text and image splits\n",
    "    collection_name=\"multi_model_rag\",\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "                search_type=\"similarity\",\n",
    "                search_kwargs={'k': 1}, # number of documents to retrieve\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fbbebe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    \"What are the main challenges and opportunities of applying large language models (LLMs) to tabular data, \"\n",
    "    \"as discussed in the 'Survey on Tabular Data'? Please summarize the key techniques, typical preprocessing steps, \"\n",
    "    \"and evaluation metrics used for LLMs working with tabular data.\"\n",
    ")\n",
    "query = \" \".join([query]) if isinstance(query, tuple) else query\n",
    "\n",
    "docs = retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c30cc3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. The transformation of tabular data into LLM-readable natural language addresses the curse of\n",
      "dimensionality (created by the one-hot encoding of categorical data).\n",
      "3. The emergent capabilities, such as step-by-step reasoning through CoT, have transformed LM from\n",
      "language modeling to a more general task-solving tool. Research is needed to test the limit of LLM’s\n",
      "emergent abilities on tabular data modeling.\n",
      "1.4\n",
      "Contribution\n",
      "The key contributions of this work are as follows:\n",
      "1. A formal break down of key techniques for LLMs’ applications on tabular data We\n",
      "split the application of LLM in tabular data to tabular data prediction, tabular data synthesis,\n",
      "tabular data question answering and table understanding. We further extract key techniques that\n",
      "can apply to all applications. We organize these key techniques in a taxonomy that researchers and\n",
      "practitioners can leverage to describe their methods, find relevant techniques and understand the\n",
      "difference between these techniques. We further subdivide each technique to subsections so that\n",
      "researchers can easily find relevant benchmark techniques and properly categorize their proposed\n",
      "techniques.\n",
      "2. A survey and taxonomy of metrics for LLMs’ applications on tabular data. For each\n",
      "application, we categorize and discuss a wide range of metrics that can be used to evaluate the\n",
      "performance of that application. For each application, we documented the metric of all relevant\n",
      "methods, and we identify benefits/limitations of each class of metrics to capture application’s per-\n",
      "formance. We also provide recommended metrics when necessary.\n",
      "3. A survey and taxonomy of datasets for LLMs’ applications on tabular data.\n",
      "For\n",
      "each application, we identify datasets that are commonly used for benchmark.\n",
      "For table un-\n",
      "{'name': 7}\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0ede340c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided text snippet from a survey on tabular data and LLMs:\n",
      "\n",
      "1.  **Main Challenges:**\n",
      "    *   The high dimensionality issue caused by one-hot encoding categorical data in tables (the \"curse of dimensionality\").\n",
      "    *   The need to represent structured tabular data in a way LLMs can process effectively (transforming tabular data into LLM-readable natural language).\n",
      "\n",
      "2.  **Main Opportunities:**\n",
      "    *   Leveraging LLMs' emergent capabilities like step-by-step reasoning (Chain-of-Thought, CoT) to move beyond language modeling and tackle more general, structured data tasks (tabular data modeling, prediction, etc.).\n",
      "\n",
      "3.  **Key Techniques:**\n",
      "    *   The survey breaks down LLM applications to specific tasks: tabular data prediction, synthesis, question answering, and table understanding.\n",
      "    *   It organizes techniques used (like natural language description generation from tables) into a taxonomy, subdividing them into subsections for clarity, comparison, and benchmarking.\n",
      "\n",
      "4.  **Typical Preprocessing Steps:**\n",
      "    *   The core preprocessing step mentioned is the transformation of tabular data into LLM-readable natural language. This implicitly involves handling categorical data (to address the dimensionality curse) and structuring the data into promptable format.\n",
      "\n",
      "5.  **Evaluation Metrics:**\n",
      "    *   The survey categorizes and discusses various metrics specific to each application (prediction, QA, etc.) to evaluate performance.\n",
      "    *   It analyzes the benefits and limitations of different metric classes for each application and provides recommendations where necessary.\n"
     ]
    }
   ],
   "source": [
    "# System prompt\n",
    "# system = \"\"\"You are an assistant for QA based on academic papers.\n",
    "# Always ground your answer strictly in the retrieved evidence below—no outside knowledge.\n",
    "# If evidence is from an image or table summary, reference the figure/table number if present.\"\"\"\n",
    "\n",
    "# prompt = ChatPromptTemplate.from_messages([\n",
    "#     (\"system\", system),\n",
    "#     (\"human\", \"Retrieved documents:\\n\\n<docs>{documents}</docs>\\n\\nUser question:\\n{question}\")\n",
    "# ])\n",
    "\n",
    "system = \"\"\"\n",
    "You are an assistant for QA on scientific papers. \n",
    "You must:\n",
    "1. You are an expert assistant answering questions about academic papers.\n",
    "2. If explaining a table or figure, mention the table/figure number (e.g., \"Table 5\") and page.\n",
    "3. Explain what the table shows *and* how it connects to the paper's main argument.\n",
    "Keep it concise .\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", system),\n",
    "  (\"human\", \"Context:\\n<docs>{documents}</docs>\\n\\nQuestion:\\n{question}\")\n",
    "])\n",
    "\n",
    "\n",
    "# LLM Initialization\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    base_url=os.getenv(\"OPENROUTER_URL\"),\n",
    "    model=os.getenv(\"MODEL_NAME\"),\n",
    "    # temperature=0.05,\n",
    "    # max_tokens=256,\n",
    "    # streaming=True,\n",
    "    # extra_body={'repetition_penalty': 1.03},\n",
    "    # top_p=0.9,\n",
    ")\n",
    "\n",
    "# Build the RAG chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Retrieval\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "docs_joined = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Run\n",
    "generation = rag_chain.invoke({\"documents\": docs_joined, \"question\": query})\n",
    "print(generation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cb18c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
