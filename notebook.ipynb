{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79a67c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langchain langchain-community pillow pymupdf python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2483536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\semal\\anaconda3\\envs\\gpu_conda\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import io\n",
    "import os\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from PIL import Image\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import google.generativeai as genai\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "from langchain_huggingface.llms.huggingface_endpoint import HuggingFaceEndpoint\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "537310a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = []\n",
    "\n",
    "img_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e73e6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with fitz.open('training_documents/Overclocking LLM Reasoning - Monitoring and Controlling Thinking Path Lengths.pdf') as pdf_file:\n",
    "    # Create a directory to store the images\n",
    "    if not os.path.exists(\"extracted_images\"):\n",
    "        os.makedirs(\"extracted_images\")\n",
    "    \n",
    "     # Loop through every page in the PDF\n",
    "    for page_number in range(len(pdf_file)):\n",
    "        page = pdf_file[page_number]\n",
    "        \n",
    "        # Get the text on page\n",
    "        text = page.get_text().strip()\n",
    "        text_data.append({\"response\": text, \"name\": page_number+1})\n",
    "        # Get the list of images on the page\n",
    "        images = page.get_images(full=True)\n",
    "\n",
    "        # Loop through all images found on the page\n",
    "        for image_index, img in enumerate(images, start=0):\n",
    "            xref = img[0]  # Get the XREF of the image\n",
    "            base_image = pdf_file.extract_image(xref)  # Extract the image\n",
    "            image_bytes = base_image[\"image\"]  # Get the image bytes\n",
    "            image_ext = base_image[\"ext\"]  # Get the image extension\n",
    "            \n",
    "            # Load the image using PIL and save it\n",
    "            image = Image.open(io.BytesIO(image_bytes))\n",
    "            image.save(f\"extracted_images/image_{page_number+1}_{image_index+1}.{image_ext}\")    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e1533a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv('API_KEY')\n",
    "client = OpenAI(\n",
    "    api_key=api_key,\n",
    "    base_url=\"https://openrouter.ai/v1\"\n",
    ")\n",
    "\n",
    "def image_to_base64(image_path, max_size=1024):\n",
    "    with Image.open(image_path) as im:\n",
    "        # Resize image if too large\n",
    "        if max(im.size) > max_size:\n",
    "            im.thumbnail((max_size, max_size))\n",
    "        # Convert to RGB if needed (some models dislike RGBA)\n",
    "        if im.mode != \"RGB\":\n",
    "            im = im.convert(\"RGB\")\n",
    "        # Save to buffer with reduced quality\n",
    "        buffer = BytesIO()\n",
    "        im.save(buffer, format=\"JPEG\", quality=70, optimize=True)\n",
    "        return base64.b64encode(buffer.getvalue()).decode()\n",
    "\n",
    "\n",
    "img_data = []\n",
    "for img in os.listdir(\"extracted_images\"):\n",
    "    img_path = f\"extracted_images/{img}\"\n",
    "    img_b64 = image_to_base64(img_path)\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"google/gemini-2.0-flash-001\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \n",
    "             \"content\": \"\"\"\n",
    "                        You are an AI assistant helping build a retrieval system from academic papers. \n",
    "                        The input is a table or figure image extracted from a paper. Summarize the image with reference to the core topic or claim being visualized. \n",
    "                        Include comparisons, axes, legends, and what this visual proves or supports in context of the paper. \n",
    "                        Your summary will be embedded and must serve as a high-quality retrieval chunk. Be specific, concise, and factually grounded.\n",
    "                        \"\"\"\n",
    "                        },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/png;base64,{img_b64}\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    # Adjust to match OpenAI format\n",
    "    img_data.append({\"response\": response, \"name\": img})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9b7d88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")\n",
    "\n",
    "# Load the document\n",
    "docs_list = [Document(page_content=text['response'], metadata={\"name\": text['name']}) for text in text_data]\n",
    "img_list = [Document(page_content=img['response'], metadata={\"name\": img['name']}) for img in img_data]\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=400, chunk_overlap=50\n",
    ")\n",
    "\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "img_splits = text_splitter.split_documents(img_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb9b7039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add to vectorstore\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits + img_splits, # adding the both text and image splits\n",
    "    collection_name=\"multi_model_rag\",\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "                search_type=\"similarity\",\n",
    "                search_kwargs={'k': 1}, # number of documents to retrieve\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbbebe93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\semal\\AppData\\Local\\Temp\\ipykernel_45956\\2626298704.py:19: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(query)\n"
     ]
    }
   ],
   "source": [
    "query = (\n",
    "    \"\"\"\n",
    "            In the \"Overclocking LLM Reasoning\" paper, there is a set of bar graphs comparing prediction errors for different prompt styles and reasoning path lengths.\n",
    "\n",
    "            Explain what these bar graphs show about the model’s ability to predict ideal progress across different settings.\n",
    "\n",
    "            Specifically:\n",
    "\n",
    "            What does the height of the bars represent?\n",
    "\n",
    "            How do different prompt styles or bin lengths affect prediction error?\n",
    "\n",
    "            What does this reveal about the robustness of the TPV (Thinking Progress Vector) model?\n",
    "                \n",
    "    \"\"\"\n",
    ")\n",
    "query = \" \".join([query]) if isinstance(query, tuple) else query\n",
    "\n",
    "docs = retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c30cc3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "show that the relative position can be captured by projections that we term “progress vectors\".\n",
      "The extracted information is then used to create an interactive loading bar visualization, see Figure 1(a)\n",
      "that depicts the model’s progress throughout the thinking phase, making the reasoning process more\n",
      "transparent and easier for users to collaborate with.\n",
      "The ability to extract progress information does not mean that the model employs it mechanistically,\n",
      "unless an intervention analysis is performed. We thus manipulate the internal representation along\n",
      "the progress vectors and achieve a clear modulation of the length of the thinking phase, showing\n",
      "overclocking effects. The former is depicted in Figure 1(b). Reassuringly, this modulation does not\n",
      "tend to be detrimental to the LLM’s performance. In fact, we show that overclocking can improve the\n",
      "model’s performance by mitigating overthinking, enhancing computational efficiency, and tailoring\n",
      "the model’s reasoning depth to each task’s complexity.\n",
      "Our main contributions are as follows: (i) We provide the first empirical evidence that LLMs\n",
      "maintain an internal estimate of their relative position within the explicit thinking phase. This finding\n",
      "sheds light on the plausibility of planning and self-monitoring abilities of LLMs, concepts typically\n",
      "associated with cognitive capabilities. (ii) We identify an internal encoding of this information by\n",
      "learning progress vector projections, and employ these projections to expose a dynamic thinking\n",
      "progress bar. (iii) We perform an intervention study and manipulate the progress vectors to overclock\n",
      "and downclock the reasoning process. Finally, (iv) We empirically demonstrate that interventions\n",
      "of the progress vectors improve both the efficiency and the effectiveness of strong LLMs such as\n",
      "DeepSeek-R1 by mitigating overthinking and reducing the generation of unnecessary reasoning steps.\n",
      "2\n",
      "Related Work\n",
      "{'name': 2}\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)\n",
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ede340c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the \"Overclocking LLM Reasoning\" paper, the set of bar graphs comparing prediction errors illustrates how well the model can estimate its ideal progress during reasoning under various prompt styles and reasoning path lengths. \n",
      "\n",
      "1. **Height of the Bars**: The height of the bars in the graphs represents the level of prediction error, with lower bars indicating better accuracy in the model's estimations of its progress within the thinking phase. \n",
      "\n",
      "2. **Effects of Different Prompt Styles and Bin Lengths**: Variations in prompt styles or bin lengths influence prediction error, where certain styles may lead to more accurate progress estimations, resulting in shorter bars (lower prediction errors). For example, more structured or specific prompts could guide the model toward its ideal reasoning trajectory more effectively than vague prompts. Similarly, bin lengths that better align with the model's reasoning process may yield lower prediction errors.\n",
      "\n",
      "3. **Robustness of the TPV Model**: The observations from these bar graphs suggest that the Thinking Progress Vector (TPV) model is robust across different conditions. The fact that the model maintains or improves its prediction accuracy with varying prompts and reasoning path lengths indicates that it has a flexible internal mechanism capable of adjusting its progress determination based on contextual cues. This adaptability reinforces the claim that the model can engage in forms of self-monitoring and planning, which are critical for effective reasoning. \n",
      "\n",
      "Overall, the bar graphs not only quantify the model's performance but also validate the theoretical framework of the TPV by showing its effectiveness under diverse settings.\n"
     ]
    }
   ],
   "source": [
    "system = \"\"\"\n",
    "You are an assistant for QA on scientific papers. \n",
    "You must:\n",
    "1. You are an expert assistant answering questions about academic papers.\n",
    "2. If explaining a table or figure, mention the table/figure number (e.g., \"Table 5\") and page.\n",
    "3. Explain what the table shows *and* how it connects to the paper's main argument.\n",
    "Keep it concise .\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "  (\"system\", system),\n",
    "  (\"human\", \"Context:\\n<docs>{documents}</docs>\\n\\nQuestion:\\n{question}\")\n",
    "])\n",
    "\n",
    "\n",
    "# LLM Initialization\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=os.getenv(\"OPENROUTER_API_KEY\"),\n",
    "    base_url=os.getenv(\"OPENROUTER_URL\"),\n",
    "    model=os.getenv(\"MODEL_NAME\"),\n",
    "    max_tokens=32000,\n",
    ")\n",
    "\n",
    "# Build the RAG chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Retrieval\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "docs_joined = \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Run\n",
    "generation = rag_chain.invoke({\"documents\": docs_joined, \"question\": query})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1c6dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
